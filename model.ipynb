{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "# Step 1: Prepare directories and create varied dummy data\n",
        "texts = []\n",
        "labels = []\n",
        "\n",
        "base_dir = \"data\"\n",
        "categories = [\"technology\", \"sports\", \"finance\", \"design_arts\", \"engineering\", \"health_medicine\", \"volunteering\", \"career_advice\", \"entrepreneurship\", \"internships_jobs\", \"study\"]\n",
        "\n",
        "category_words = {\n",
        "    \"technology\": [\"computer\", \"smartphone\", \"app\", \"software\", \"hardware\", \"internet\", \"AI\", \"robot\", \"gadget\", \"coding\", \"programming\", \"blockchain\", \"cybersecurity\", \"data\", \"cloud\"],\n",
        "    \"sports\": [\"cricket\", \"football\", \"basketball\", \"tennis\", \"swimming\", \"running\", \"match\", \"game\", \"player\", \"team\", \"coach\", \"stadium\", \"olympics\", \"fitness\", \"exercise\"],\n",
        "    \"finance\": [\"stock\", \"market\", \"investment\", \"bond\", \"currency\", \"bank\", \"loan\", \"budget\", \"economy\", \"trade\", \"saving\", \"cost\", \"planning\", \"retirement\", \"crypto\"],\n",
        "    \"design_arts\": [\"art\", \"design\", \"painting\", \"sculpture\", \"graphic\", \"architecture\", \"fashion\", \"drawing\", \"photography\", \"illustration\", \"creative\", \"museum\", \"gallery\"],\n",
        "    \"engineering\": [\"engineer\", \"build\", \"machine\", \"machine\", \"circuit\", \"bridge\", \"robotics\", \"mechanical\", \"electrical\", \"civil\", \"chemical\", \"aerospace\", \"software\", \"project\"],\n",
        "    \"health_medicine\": [\"health\", \"medicine\", \"doctor\", \"hospital\", \"disease\", \"treatment\", \"fitness\", \"nutrition\", \"exercise\", \"wellness\", \"mental\", \"vaccine\", \"surgery\", \"diet\"],\n",
        "    \"volunteering\": [\"volunteer\", \"help\", \"community\", \"charity\", \"donate\", \"service\", \"aid\", \"support\", \"nonprofit\", \"event\", \"cause\", \"impact\", \"organization\"],\n",
        "    \"career_advice\": [\"career\", \"job\", \"resume\", \"interview\", \"promotion\", \"salary\", \"skill\", \"networking\", \"mentorship\", \"development\", \"goal\", \"advice\", \"path\"],\n",
        "    \"entrepreneurship\": [\"startup\", \"business\", \"entrepreneur\", \"idea\", \"venture\", \"innovation\", \"funding\", \"market\", \"product\", \"growth\", \"strategy\", \"leadership\"],\n",
        "    \"internships_jobs\": [\"internship\", \"job\", \"application\", \"position\", \"hiring\", \"employer\", \"experience\", \"opportunity\", \"role\", \"company\", \"training\", \"placement\"],\n",
        "    \"study\": [\"study\", \"learn\", \"education\", \"school\", \"university\", \"exam\", \"homework\", \"course\", \"knowledge\", \"research\", \"book\", \"lecture\", \"degree\"]\n",
        "}\n",
        "\n",
        "templates = [\n",
        "    \"I am interested in {word}\",\n",
        "    \"Discussing {word} with friends\",\n",
        "    \"My experience with {word}\",\n",
        "    \"Attended an event about {word}\",\n",
        "    \"Learned new things on {word}\",\n",
        "    \"Shared thoughts on {word}\",\n",
        "    \"The importance of {word}\",\n",
        "    \"How to get started with {word}\",\n",
        "    \"Tips for {word}\",\n",
        "    \"Challenges in {word}\",\n",
        "    \"Latest news on {word}\",\n",
        "    \"My favorite {word}\",\n",
        "    \"Exploring {word}\",\n",
        "    \"Questions about {word}\"\n",
        "]\n",
        "\n",
        "for category in categories:\n",
        "    os.makedirs(os.path.join(base_dir, category), exist_ok=True)\n",
        "\n",
        "for category in categories:\n",
        "    csv_path = os.path.join(base_dir, category, f\"{category}_posts_1000.csv\")\n",
        "    words = category_words[category]\n",
        "    posts = []\n",
        "    for i in range(1000):\n",
        "        template = np.random.choice(templates)\n",
        "        word = np.random.choice(words)\n",
        "        post = template.format(word=word)\n",
        "        posts.append(post)\n",
        "    dummy_data = {'post': posts}\n",
        "    dummy_df = pd.DataFrame(dummy_data)\n",
        "    dummy_df.to_csv(csv_path, index=False)\n",
        "\n",
        "print(\"Varied dummy data directories and files created.\")\n",
        "\n",
        "# Step 2: Load data from CSV files\n",
        "for idx, category in enumerate(categories):\n",
        "    csv_path = os.path.join(base_dir, category, f\"{category}_posts_1000.csv\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "    for text in df[\"post\"]:\n",
        "        texts.append(text)\n",
        "        labels.append(idx)\n",
        "\n",
        "print(f\"Loaded {len(texts)} texts with {len(set(labels))} labels\")\n",
        "\n",
        "# Step 3: Split data into train and test sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    texts, labels, test_size=0.2, stratify=labels, random_state=42\n",
        ")\n",
        "\n",
        "# Step 4: Set parameters and create TextVectorization layer\n",
        "max_features = 10000  # Size of vocabulary\n",
        "sequence_length = 50  # Max number of words per sample\n",
        "\n",
        "vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length\n",
        ")\n",
        "\n",
        "# Step 5: Adapt vectorizer and prepare datasets\n",
        "vectorize_layer.adapt(train_texts)\n",
        "\n",
        "train_texts_ds = tf.data.Dataset.from_tensor_slices((train_texts, train_labels))\n",
        "test_texts_ds = tf.data.Dataset.from_tensor_slices((test_texts, test_labels))\n",
        "\n",
        "def vectorize_text(text, label):\n",
        "    return vectorize_layer(text), label\n",
        "\n",
        "train_ds = train_texts_ds.map(vectorize_text)\n",
        "test_ds = test_texts_ds.map(vectorize_text)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = train_ds.shuffle(10000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Step 6: Build and compile the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(max_features + 1, 16),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(11)  # For 11 classes\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Step 7: Train the model\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=test_ds,\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "# Step 8: Create export model with softmax\n",
        "export_model = tf.keras.Sequential([\n",
        "    vectorize_layer,\n",
        "    model,\n",
        "    tf.keras.layers.Activation('softmax')\n",
        "])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Step 9: Define prediction function\n",
        "def predict_category(text):\n",
        "    input_tensor = tf.constant([text])\n",
        "    probs = export_model.predict(input_tensor)[0]\n",
        "\n",
        "    predicted_index = np.argmax(probs)\n",
        "    predicted_category = categories[predicted_index]\n",
        "\n",
        "    prob_percentages = {cat: f\"{prob * 100:.2f}%\" for cat, prob in zip(categories, probs)}\n",
        "\n",
        "    print(f\"Predicted Category: {predicted_category}\")\n",
        "    print(\"Probabilities:\")\n",
        "    for cat, perc in prob_percentages.items():\n",
        "        print(f\" - {cat}: {perc}\")\n",
        "\n",
        "# Step 10: Test predictions\n",
        "print(predict_category(\"Played a great match of cricket with friends in the main ground.\"))\n",
        "\n",
        "print(predict_category(\"Attended event on Budget planning cost saving as a student\"))\n",
        "\n",
        "# Step 11: Prepare for TFLite conversion\n",
        "@tf.function(input_signature=[tf.TensorSpec(shape=[None, 50], dtype=tf.int32)])\n",
        "def prob_model_wrapper(input):\n",
        "    logits = model(input)\n",
        "    return tf.nn.softmax(logits)\n",
        "\n",
        "concrete_func = prob_model_wrapper.get_concrete_function()\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Step 12: Save TFLite model\n",
        "with open(\"model_with_softmax.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "# Step 13: Load and test TFLite model\n",
        "interpreter = tf.lite.Interpreter(model_path=\"model_with_softmax.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "print(\"Input shape:\", input_details[0]['shape'])\n",
        "print(\"Input dtype:\", input_details[0]['dtype'])\n",
        "\n",
        "input_index = input_details[0]['index']\n",
        "output_index = output_details[0]['index']\n",
        "\n",
        "sample_text = \"Attended event on Budget planning cost saving as a student\"\n",
        "vec = vectorize_layer(tf.constant([sample_text]))  # shape (1, 50)\n",
        "vec = tf.cast(vec, tf.int32)\n",
        "\n",
        "interpreter.set_tensor(input_index, vec.numpy())\n",
        "interpreter.invoke()\n",
        "probs = interpreter.get_tensor(output_index)[0]\n",
        "\n",
        "for cat, prob in zip(categories, probs):\n",
        "    print(f\"{cat}: {prob * 100:.2f}%\")\n",
        "\n",
        "print(\"Predicted Category:\", categories[np.argmax(probs)])\n",
        "\n",
        "# Step 14: Save vocabulary and labels\n",
        "vocab = vectorize_layer.get_vocabulary()\n",
        "\n",
        "with open(\"vocab.txt\", \"w\") as f:\n",
        "    for token in vocab:\n",
        "        f.write(token + \"\\n\")\n",
        "\n",
        "with open(\"labels.txt\", \"w\") as f:\n",
        "    for label in categories:\n",
        "        f.write(label + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeaDzLZ8XGgv",
        "outputId": "edba1384-34c5-480e-9c70-bf129dcc2d8f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.19.0\n",
            "Varied dummy data directories and files created.\n",
            "Loaded 11000 texts with 11 labels\n",
            "Epoch 1/10\n",
            "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.0953 - loss: 2.3978 - val_accuracy: 0.0909 - val_loss: 2.3908\n",
            "Epoch 2/10\n",
            "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.1248 - loss: 2.3895 - val_accuracy: 0.1423 - val_loss: 2.3801\n",
            "Epoch 3/10\n",
            "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.2054 - loss: 2.3763 - val_accuracy: 0.2836 - val_loss: 2.3618\n",
            "Epoch 4/10\n",
            "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.2305 - loss: 2.3564 - val_accuracy: 0.3800 - val_loss: 2.3318\n",
            "Epoch 5/10\n",
            "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.3712 - loss: 2.3226 - val_accuracy: 0.2864 - val_loss: 2.2880\n",
            "Epoch 6/10\n",
            "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4378 - loss: 2.2730 - val_accuracy: 0.6514 - val_loss: 2.2232\n",
            "Epoch 7/10\n",
            "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5508 - loss: 2.2048 - val_accuracy: 0.6727 - val_loss: 2.1448\n",
            "Epoch 8/10\n",
            "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7251 - loss: 2.1166 - val_accuracy: 0.8777 - val_loss: 2.0436\n",
            "Epoch 9/10\n",
            "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8194 - loss: 2.0139 - val_accuracy: 0.9591 - val_loss: 1.9250\n",
            "Epoch 10/10\n",
            "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8753 - loss: 1.8923 - val_accuracy: 0.9282 - val_loss: 1.8061\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "Predicted Category: sports\n",
            "Probabilities:\n",
            " - technology: 6.80%\n",
            " - sports: 27.75%\n",
            " - finance: 10.77%\n",
            " - design_arts: 5.50%\n",
            " - engineering: 7.81%\n",
            " - health_medicine: 4.43%\n",
            " - volunteering: 8.17%\n",
            " - career_advice: 8.79%\n",
            " - entrepreneurship: 7.16%\n",
            " - internships_jobs: 8.60%\n",
            " - study: 4.23%\n",
            "None\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Please consider providing the trackable_obj argument in the from_concrete_functions. Providing without the trackable_obj argument is deprecated and it will use the deprecated conversion path.\n",
            "WARNING:tensorflow:Issue encountered when serializing table_initializer.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'NoneType' object has no attribute 'name'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Category: finance\n",
            "Probabilities:\n",
            " - technology: 3.64%\n",
            " - sports: 6.00%\n",
            " - finance: 57.62%\n",
            " - design_arts: 2.57%\n",
            " - engineering: 0.96%\n",
            " - health_medicine: 1.48%\n",
            " - volunteering: 7.81%\n",
            " - career_advice: 6.59%\n",
            " - entrepreneurship: 4.23%\n",
            " - internships_jobs: 3.73%\n",
            " - study: 5.36%\n",
            "None\n",
            "Input shape: [ 1 50]\n",
            "Input dtype: <class 'numpy.int32'>\n",
            "technology: 3.64%\n",
            "sports: 6.00%\n",
            "finance: 57.62%\n",
            "design_arts: 2.57%\n",
            "engineering: 0.96%\n",
            "health_medicine: 1.48%\n",
            "volunteering: 7.81%\n",
            "career_advice: 6.59%\n",
            "entrepreneurship: 4.23%\n",
            "internships_jobs: 3.73%\n",
            "study: 5.36%\n",
            "Predicted Category: finance\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        }
      ]
    }
  ]
}